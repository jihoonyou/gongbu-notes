# day0329

## 강의 정리

### 3강
#### Dataset
- 주어진 데이터가 있는데 왜 데이터셋이 필요할까?
    - (Vanilla)데이터는 파일에 있는 형태
    - model을 위한 데이터 형태의 dataset으로 변환이 필요
#### Pre-preocessing
- Competition data는 데이터가 잘 정의되어잇지만, 실제 데이터는 그렇지 않다
- bounding box
    - 가끔 필요 이상으로 많은 정보를 가지고 있다
    - target외에 나머지는 noise data로
- resize
    - 계산의 효율을 위해 적당한 크기로 사이즈 변경
    - 너무 좋은 카메라(DSLR)로 찍은 사진의 경우, 화질이 너무 좋음(픽셀이 큰 듯)
    - resize하여 빠른 학습이 진행될 수 있도록 최적화 (성능은 많이 안떨어지는듯?)
- 전처리 과정을 통해 더 좋은 결과를 낼 수 있도록 할 수 있다.
#### Generalization
- Bias & Variance
    - High Bias(Underfitting): 학습이 적게 된 것
    - High Variance(Overfitting): noise까지 고려하게 되는 것
- Train / Validation
    - Train set을 validation set으로 나누어서 일반화가 됐는지 확인하는 것
- Data Augmentation
    - 예측해야되는 이미지의 다양한 variance를 추출하는 것
    - 데이터에 noise가 추가되어 들어오겠다는 가정을 시작으로 augmentation을 시작!
    - ex) 원본 이미지를 명도를 낮추어 밤 데이터를 얻거나 폭우 데이터를 얻는 것
    - 문제가 만들어진 배경과 모델의 쓰임새에 주의하여 힌트를 얻음
- torchvision.transforms
    - 이미지에 다양한 함수들이 적용될 수 있지만..
    - 무조건적으로 적용하지 말자 
        - ex) Flip: 사진데이터를 굳이 거꾸로? (현 마스크 도메인에서는 필요 없음)
- Albumentations을 많이 사용하는듯? (특징 빠르고, 다양함)
- **무조건**이라는 단어를 제일 조심하세요
    - 함수들을 무조건 적용하는 것은 옳지 않음
    - 앞서 정의한 Problem(주제)을 깊이 관찰해서 어떤 기법을 적용하면 좋겠다라는 가정을 갖고 실험으로 증명

### 4강
#### Data Generation
> 효율이 안 나올 경우..
> 좀 더 빠른 training과 inference를 할 수 있게
- 데이터 셋을 잘 구성했더라도, 잘 출력해야한다

#### Data Feeding
- 먹이를 주다: 대상의 상태를 고려해서 적정한 양을 준다
- 공정 예시
    - 어떤 제품을 생성할 때, 중간단게를 고려하여 생산량을 늘려 효율적으로 할 수 있다
- 보통은 모델의 처리량을 바꾸기 힘들다. 즉, 모델이 처리할 수 있는 양만큼 data를 Generate하는 것이 중요하다
- Dataset 생성 능력 비교
    - dataset에서 transforms할 때 sequence에 따라 데이터 생성 속도의 차이가 있다
        - 직접 실험을 해서 좋은 속도를 찾으면 좋을 것 같다.
        - 예시로는 randomrotation한 후, resize를 하는 것이 빨랐다
        - 아마 사이즈가 큰 것을 돌리는 것보다, 돌리고 난 후 사이즈를 늘리는 것의 차이였을지도..

#### torch.utils.data
> Dataset
> DataLoader
- Dataset: 모델에 필요한 형태로 데이터를 구성하는 것
- Dataset의 구조는 파이썬 경험으로 패스
```
from torch.utils.data import Dataset

class MyDataset(Dataset):
    def __init__(self):
        pass
    def __getitem(self, index):
        return ___
    def __len__(self):
        return len(___)
```
- DataLoader: 내가 만든 Dataset을 효율적으로 사용할 수 있도록 관련기능 추가
    - batch_size, num_workers ...
    - collate_fn: 배치마다 다른 것을 적용하고 싶을 때 사용
    - num_workers의 경우.. 무조건 큰 값이 아닌, 현재 컴퓨팅 환경에 맞춰서 적용해야한다!
        - 이 부분은 속도 테스트를 하면 꽤나 재밌을듯 ㅎㅎ
- Dataset과 DataLoader의 분리가 필요함 (재활용 측면에서 둘이 하는 일이 다르기 때문)

## 목표
- 베이스 라인 코드를 다시 작성하여 완성을 해본다
- pretrained모델로 Efficientnet을 사용했는데, 학습 시간이 오래거려서 많은 결과를 얻지 못하는 것 같다. (한번에 좋은 결과를 내는 실력이 부족함) 
    - ResNet으로 바꾸어 여러번 submission하는 것을 목표로 한다.
- 개인 공부로는 게임서버 책 읽기
- 네트워크 정리(Application Layer, Transport Layer, Network Layer 까지)

## 행동
- 베이스 라인 코드를 작성하여 submission을 해보았다.
    - ResNet으로 바꾸어 결과를 얻음

## 회고
- 많은분들이 submission을 하셨고, 성능 향상을 위해 노력을 하시는 것 같다. 내일은 daily mission을 돌아보며 submission을 하며 미쳐 생각하지 못햇던 부분을 채워야겠다.