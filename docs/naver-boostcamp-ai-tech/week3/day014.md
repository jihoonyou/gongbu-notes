# Recurrent Neural Networks

## RNN 첫걸음
> 시계열 데이터, sequence data

### 시퀀스 데이터 이해하기
> 소리, 문자열, 주가 등의 데이터를 시퀀스(sequence) 데이터로 분류합니다.
> 시퀀스 데이터는 독립동등분포(i.i.d) 가정을 잘 위배하기 때문에 순서를 바꾸거나 과거정보에 손실이 발생하면 데이터의 확률분포도 바뀌게 됩니다
- 과거 정보 또는 앞뒤 맥락 없이 미래를 예측하거나 문장을 완성하는 건 불가능하다
- 개가 사람을 물었다
- 사람이 개를 물었다 - 의미가 바뀌고, 빈도도 적음
- 순차적으로 들어오는 정보를 어떻게 다룰지 고민해야함

### 시퀀스 데이터를 어떻게 다루나요?
> 이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부확률을 이용할수 있습니다.
![seq_data_func](../../images/seq_data_func.png)
- 위 조건부확률은 과거의 모든 정보를 사용하지만 시퀀스 데이터를 분석할 때 모든 과거 정보들이 필요한 것은 아닙니다
    - 주가 예측시, 창립되었던 년도부터가 아닌 5년에서 최근 정보로 예측
> 시퀀스 데이터를 다루기 위해선 길이가 가변적인 데이터를 다룰 수 있는 모델이 필요합니다
- Xt 예측, Xt+1 예측
![autoregressive_model](../../images/autoregressive_model.png)
- tao를 정해야하고, tao도 바뀔 수 있음
![latent_v](../../images/latent_v.png)
- 직전의 정보와 잠재변수만 갖고 모델링
    - 잠재변수를 어떻게 인코딩? - RNN
![l_v_rnn](../../images/l_v_rnn.png)

### Recurrent Neural Network 이해하기
> 가장 기본적인 RNN 모형은 MLP와 유사한 모양입니다
![rnn_func](../../images/rnn_func.png)
- Xt -> Ht -> Ot
    - 현재 시점 데이터 t만 들어오기 때문에 이 모델은 과거의 정보를 다룰 수 없음

> RNN은 이전 순서의 잠재변수와 현재의 입력을 활용하여 모델링합니다
![rnn_func2](../../images/rnn_func2.png)
    - Wx1:입력 데이터에서
    - WH: 이전 시점의 잠재변수에서
    - W2: 새로만들어진 잠재변수를 
        - 3개의 가중치 행렬은 t에 따라 변하는게 아님
        - t에따라 변하는 것은 잠재변수와 입력 데이터

> RNN의 역전파는 잠재변수의 연결그래프에 따라 순차적으로 계산합니다
![rnn_backpropagation](../../images/rnn_backpropagation.png)
- BPTT(Backpropagation Through Time) Xt까지 예측이 일어난 다음에 맨 과거부터 타고 흘러와서 
    - 잠재변수에서 들어오는 gradient vector와 출력에서 들어오는 gradient vector
        - 이 gradient vector를 입력에 전달! 이것을 반복해서 학습이 됨

### BPTT를 좀 더 살펴봅시다
> BPTT 를 통해 RNN의 가중치행렬의 미분을 계산해보면 아래와 같은 비분의 곱으로 이루어진 항이 계산됩니다.
![bptt](../../images/rnn_backpropagation.png)
- sequence 길이가 길어질수록 이 항은 불안정해지기 쉽습니다
    - 모든 t시점에서 적용하면 불안정해기 쉬움

### 기울기 소실의 해결책
> 시퀀스 길이가 길어지는 경우 BPTT를 통한 역전파 알고리즘이 계산이 불안정해지므로 길이를 끊는 것이 필요합니다
- gradient가 0으로 줄어드는 문제
    - 미래 시점에 갈수록 gradient가 살아있고 과거시점에서 0으로 사라지면.. 과거정보 유실
        - 문맥적으로 이전 시점이 필요한 text분석 또는 긴 sequence분석에 문제가 생김
- 해결책: truncated BPTT
    - BPTT의 모든 시점을 계산하는 것이 아닌 일부를 끊고 
    - Ht는 Ot에서만 gradient를 받음
    - 특정 block에서 끊고 gradient를 받는 것

> 이런 문제들 때문에 Vanilla RNN 은 길이가 긴 시퀀스를 처리하는데 문제가 있습니다
![lstm_gru](../../images/lstm_gru.png)
- 오늘 배운 RNN으로 모델을 만들지 않음

## Sequential Models - RNN
> 그 전까지는 이미지에 대한 것, MLP는 벡터를 다른 벡터로 바꾸는 것, CNN는 이미지를 원하는 형태로 (classification: 원하는 class의 개수로, detection: 출력값이 각각의 영역에 있는 bounding box를 찾는 것, semantics: 이미즤 픽셀별로 어떤 클래스에 속하는지 찾는 것)
> RNN은 주어진 입력이 Sequential


### Sequntial Model
> Naive sequence model
![sm_input](../../images/sm_input.png)
- 말, 비디오, 동작, 모션 다 sequential data
- 어려운 것은 하나의 분류.
    - 길이가 정해져있지 않기 때문에.. 어려움이 있음
    - 하지만 sequential data의 길이를 알 수 없음
    - 즉 모델의 모양에 상관없이 동작해야함
- 입력이 들어올 때 다음 입력을 예측

> Autogressive model
![autogressive_model](../../images/autogressive_model.png)
- fix the past timespan
    - 예시) 과거의 5개만 보겟다

> Markov model (first-order autoregressive model)
![markov_model](../../images/markov_model.png)
- 내가 현재를 가정하기 위해 바로 전 과거를 봄
    - ex) 내일의 수능점수는 전 날 공부 한 것만 연관이 있다 (말이 안됨)

> Latent autogressive model
![latent_autogressive_model](../../images/latent_autogressive_model.png)
- 중간에 hidden state가 과거의 정보를 요약하고 있고, 현재 모델은 하나의 과거 요약본만 연관있는 것
    - output 모델에서는 하나의 과거만 보는데 그 과거는 요약정보

### Recurrent Neural Network
![rnn_pic](../../images/rnn_pic.png)
- 자기 자신으로 돌아오는 구조가 하나 있음
    - Ht는 Xt와 이전의 얻어진 self-state에도 연관이 있음
- RNN의 time을 fix하고 쭉 풀어보면, input width가 많은 네트워크

> short term dependencies
- 멀리있는 정보는 고려하기 힘들다 (long term dependencies)
    - 과거에 정보가 미래에까지 살아남기가 힘들다

![rnn_structure](../../images/rnn_structure.png)
    - activaition function이 sigmoid라면 vaninshing gradient
    - 만약에 LeRu를 쓰면 exploding gradient (너무 커짐)

### Long Short Term Memory
> 기존의 RNN을 Vanilla RNN이라고 명시
![lstm_pic](../../imagse/lstm_pic.png)
![lstm_detail](../../images/lstm_detail.png)
- Xt: input (language model에서는 단어)
- Ht: output (hidden state)
- previous cell state (내부에서만 있고, 0~t까지의 정보를 취합해준 것)
- 입력으로 들어가는건 input, previous cell state, previous hidden state, 나가는 것도 세개 하지만 실제로 나가는건 Ht만!
- LSTM은 세개의 게이트로 이루어져 있음
    - forget gate
    - input gate
    - output gate

> Core idea
- 중간에 흘러가는 cell state가 중요!
    - time step t까지 들어온 정보를 summarlize
    - 컨베이너 벨트에서 어떤 정보가 다음에 필요할지 넘기면서 조작

- forget gate
    - decide which information to **throw** away
    - 입력으로 ht-1, xt가 들어가고 ft의 결과값을 내고, sigmoid함수를 통과하기 때문에 항상 0~1 사이 값
    - 이전의 cell state의 정보 중 어떤 것을 버리고 어떤 것을 킵할지 정함
        - ht-1, xt로 정함

- Input gate
    - decide which information to **store** in the cell state
    - 현재 입력(박스)가 들어왔는데 무조건 cell state에 올리는 것이 아닌 뭐를 올리지 정하는 것
        - it - 어떤 정보를 올리고..
        - Ct - cell state candidate (현재 인풋과 이전 정보로 만들어진 cell state 예비군)
        - h-1,xt로 cell state cadidates를 새로운 스테이트에 업데이트 해야함

- Update Cell
    - update the cell state
    - Ct = ft*ct-1 + it*ct^
    - 두 값을 combine해서 cell update

- Output Gate
    - Make output using the updated cell state
    - 앞에서 취합한 정보를 한번 더 조작해서 어떤 값을 밖으로 빼낼지 정함

### Gated Recurrent unit
![GRU](../../images/GRU.png)
> Simpler architecture with two gates (reset gate and update gate)
> No cell state, just hidden state

- LTGM보다 GRU의 performance가 대게는 더 좋음
- 하지만 요즘은 transformer 구조가 나오면서 RNN 구조 둘 다 잘 안씀

## Sequential Models - Transformer

### Sequential Model
> What makes sequential modeling a hard problem to handle
![why_hard](../../images/why_hard.png)

### Transformer
> Transfomer is the first sequence transductino model based entirely on attention
- attention이라는 구조를 사용

> From a bird's-eye view, this is what the Transformer does for machine translation tasks
- 단순히 기계어 분류뿐 아니라, 이미지 분류 등 다양한 분야에도 사용
- [transfomer](http://jalammar.github.io/illustrated-transformer/)
- [참고](https://www.youtube.com/watch?v=mxGCEWOxfe8&feature=youtu.be)

> If we glide down a little bit, this is what the Transformer does
![transfomer_structure](../../images/transfomer_structure.png)
- RNN에서는 세개의 단어가 들어가면 3번 무언가 돌아감
- Transformer는 한번에 돌아감 (n개의 단어를 한번에 처리)
- 동일한 구조를 갖지만, 서로 다른 encoders와 decoders가 stacked
1. n개의 단어가 어떻게 한번에 처리 되는지
2. decoder와 encoder사이에 어떤 정보를 주고 받는지
3. decoder가 어떻게 generation 할 수 있는지

![encoder_transformer](../../images/encoder_transformer.png)

> First we represent each word with some embedding vectors

> Then, Transformer encodes each word to feature vectors with self-attention
![encoder_transformer2](../../images/encoder_transformer2.png)
- z1을 찾을 때 x1, x2, x3을 다 고려함
- 하지만, feed forward는 각자

- Self-Attention at High level
![self-attention](../../images/self-attention.png)

- Suppose we are encoding two words:
    - Thinking and Machines
![self-attention2](../../images/self-attention2.png)
- 세가지 벡터를 만듦

- encoding하고자하는 query 벡터와 나머지 vector들의 keys 벡터들의 사이를 다 내적을 함
    - i번째단어와 나머지 단어들의 사이에 iteraction이 많이 일어나야 하는지 
- then we compute the attention weights by scaling followed by softmax
- 정리
1. embedding vector가 주어지면 각각의 embedding vector마다 queries, keys, values 벡터를 만듦
2. 그리고 keys와 values의 내적으로 score를 만듦
3. normalize하고
4. sqaure of dk로 나누고 
5. softmax를 통하고
6. 사용할 것은: 각 단어에서 나오는 value vector들의 weighted sum

![transformer_calc](../../images/transformer_calc.png)
![transformer_calc2](../../images/transformer_calc2.png)
- 왜 잘될까?
    - (CNN,MLP) input이 fixed되있으면 출력이 고정
    - (Transformer) input이 fixed되있어도, 내가 encoding한 값에 따라 출력이 달라짐
        - 입력이 고정되더라도 옆에 다른 입력이 달라짐에 따라 출력도 달라짐
        - n개의 단어를 한번에 처리해야하기때문에 n^2 => 한번에 처리할 수 있는 한계가 있음 (하지만 more flexible한 네트워크를 만들 수 있음)

> multi-headed attention allows Transformer to focus on differnt position
> If eight heads are used, we end up getting eight different sets of encoded vectors (attention heads)
> We simply pass them through additional linear map
![mla](../../images/mla.png)

> Why do we need positional encoding

### Decoder
> Transformer transfer key (K) and value (V) of the topmost encoder to the decoder
> The output sequence is generated in an autogressive manner
> In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence which is doen by masking future positions before the softmax step.
> The Encoder-Decoder Attentino layer works just like multi-headed self-attention, except it creates its Queries matrix from the layer below it, and takes the keys and values from the encoder stack
> The final layer converts the stack of decoder outputs to the distribution over words

### Vision Transformer
- 최근에는 단어 외에 이미지에도 사용

### DALL-E
- 문장으로 이미지를 형성

## 개인공부
### [attention](https://nlpinkorean.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- Sequence-to-sequence (Seq2seq) 모델은 기계 번역, 문서 요약, 그리고 이미지 캡셔닝 등의 문제에서 아주 큰 성공을 거둔 딥러닝 모델
    - Seq2seq 모델은 글자, 단어, 이미지의 feature 등의 아이템 시퀀스 를 입력으로 받아 또 다른 아이템의 시퀀스를 출력
    - 입력은 일련의 단어로 이루어진 sequence 이며 맨 앞 단어부터 차례대로 모델에서 처리됩니다. 그리고 출력으론 비슷한 형태의 그러나 다른 언어로의 단어 sequence 가 나오게 됩니다
    - encoder와 decoder로 이루어져 있음
        - encoder: 입력의 각 아이템을 처리하여 거기서 정보를 추출한 후 그것을 하나의 벡터로 만듦 (context라고 불림), 입력의 모든 단어에 대한 처리가 끝난 후 encoder 는 context를 decoder 에게 보내고 출력할 아이템이 하나씩 선택되기 시작
        - context: float으로 이루어진 하나의 벡터 (마지막 hidden state)
    - encoder에서 단어들을 처리하기 전에 먼저 벡터들로 변환해주어야 합니다 (word embedding)
- context vector가 bottleneck for these types of models => Attention allows the model to focus on the relevant parts of the input sequence as needed
- Attention
    - 영어 번역을 생성하려 할 때 decoder가 스텝마다 더 집중할 수 있게 해줌
    - seq2seq와 차이점
        1. 기존 seq2seq 모델에서는 그저 마지막 아이템의 hidden state 벡터를 넘겼던 반면 attention 모델에서는 모든 스텝의 hidden states를 decoder에게 넘겨줍니다
        2. attention 모델의 decoder가 출력을 생성할 때에는 하나의 추가 과정이 필요
            1. encoder 에서 받은 전체 hidden states을 봅니다 -- 각 스텝에서의 encoder hidden states는 이전의 맥락에 대한 정보도 포함하고 있지만 그 스텝에서의 입력 단어와 가장 관련이 있습니다.
            2. 각 스텝의 hidden state마다 점수를 매깁니다 (일단 지금은 어떻게 점수를 매기는지에 대해서는 얘기하지 않겠습니다)
            3. 매겨진 점수들에 softmax를 취하고 이것을 각 타임 스텝의 hidden states에 곱해서 더합니다. 이를 통해 높은 점수를 가진 hidden states는 더 큰 부분을 차지하게 되고 낮은 점수를 가진 hidden states는 작은 부분을 가져가게 됩니다.
            - 이러한 점수를 매기는 과정은 decoder가 단어를 생성하는 매 스텝마다 반복
    
### [The Illustrated Transformer](https://nlpinkorean.github.io/illustrated-transformer/)
- multi-head self-attention을 이용해 sequential computation 을 줄여 더 많은 부분을 병렬처리가 가능하게 만들면서 동시에 더 많은 단어들 간 dependency를 모델링 한다는 것
- 가장 큰 장점은 병렬처리에 관한 부분
- Attention is all you need에서 소개됨
- Transformer
    - Encoders
        - self-attention layer
        - feed forward neural network
    - Decoders
        - self-attention layer
        - encoder-decoder attention
        - feed forward neural network
- encoder는 입력으로 벡터들의 리스트를 받습니다. 이 리스트를 먼저 self-attention layer에, 그다음으로 feed-forward 신경망에 통과시키고 그 결과물을 그다음 encoder에게 전달
    - Self-attention 층에서 이 위치에 따른 path들 사이에 다 dependency가 있습니다
    - 반면 feed-forward 층은 이런 dependency가 없기 때문에 feed-forward layer 내의 이 다양한 path 들은 병렬처리될 수 있습니다
- self-attention
    1. 각 단어에 대해서 Query vector, Key vector, Value vector 생성
        -  입력 벡터에 대해서 세 개의 학습 가능한 행렬들을 각각 곱함으로써 만들어짐
    2. 점수를 계산하는 것
        - 우리는 이 단어와 입력 문장 속의 다른 모든 단어들에 대해서 각각 점수를 계산하여야 합니다. 이 점수는 현재 위치의 이 단어를 encode 할 때 다른 단어들에 대해서 얼마나 집중을 해야 할지를 결정
        - 점수는 현재 단어의 query vector와 점수를 매기려 하는 다른 위치에 있는 단어의 key vector의 내적으로 계산됩니다
            - 우리가 위치 #1에 있는 단어에 대해서 self-attention 을 계산한다 했을 때, 첫 번째 점수는 q1과 k1의 내적일 것입니다. 그리고 동일하게 두 번째 점수는 q1과 k2의 내적일 것입니다
    3. 세 번째와 네 번째 단계는 이 점수들을 8로 나누는 것
        - 이 8이란 숫자는 key 벡터의 사이즈인 64의 제곱 근이라는 식으로 계산이 된 것입니다. 이 나눗셈을 통해 우리는 더 안정적인 gradient를 가지게 됩니다. 그리고 난 다음 이 값을 softmax 계산을 통과시켜 모든 점수들을 양수로 만들고 그 합을 1으로 만들어 줍니다
        - 이 softmax 점수는 현재 위치의 단어의 encoding에 있어서 얼마나 각 단어들의 표현이 들어갈 것인지를 결정
    5. 이제 입력의 각 단어들의 value 벡터에 이 점수를 곱하는 것
        - 이것을 하는 이유는 우리가 집중을 하고 싶은 관련이 있는 단어들은 그래도 남겨두고, 관련이 없는 단어들은 0.001 과 같은 작은 숫자 (점수)를 곱해 없애버리기 위함
    6. 이 점수로 곱해진 weighted value 벡터들을 다 합해 버리는 것
        - 이 단계의 출력이 바로 현재 위치에 대한 self-attention layer의 출력이 됩니다
    - 그러나 실제 구현에서는 빠른 속도를 위해 이 모든 과정들이 벡터가 아닌 행렬의 형태로 진행됩니다. 우리는 이때까지 각 단어 레벨에서의 계산과 그 이유에 대해서 얘기해봤다면, 이제 이 행렬 레벨의 계산을 살펴보도록 하겠습니다
- self-attentino의 행렬 계산
    1. 입력 문장에 대해서 Query, Key, Value 행렬들을 계산하는 것
        - 이를 위해 우리는 우리의 입력 벡터들 (혹은 embedding 벡터들)을 하나의 행렬 X로 쌓아 올리고, 그것을 우리가 학습할 weight 행렬들인 WQ, WK, WV 로 곱합니다
    2. 우리는 현재 행렬을 이용하고 있으므로 앞서 설명했던 self-attention 계산 단계 2부터 6까지를 하나의 식으로 압축할 수 있습니다
- multi-headed attention
    1. 모델이 다른 위치에 집중하는 능력을 확장시킵니다
    2. attention layer 가 여러 개의 “representation 공간”을 가지게 해줍니다
    - 이 8개의 행렬을 바로 feed-forward layer으로 보낼 수 없다는 것입니다. feed-forward layer 은 한 위치에 대해 오직 한 개의 행렬만을 input으로 받을 수 있습니다. 그러므로 우리는 이 8개의 행렬을 하나의 행렬로 합치는 방법을 고안해 내야 합니다    
    - 어떻게 할 수 있을까요? 간단합니다. 일단 모두 이어 붙여서 하나의 행렬로 만들어버리고, 그다음 하나의 또 다른 weight 행렬인 W0을 곱해버립니다
- tranformer 모델의 positinal encoding
    - 각각의 입력 embedding에 positional encoding이라는 벡터를 추가
        - 이 값들을 단어들의 embedding에 추가하는 것이 query/key/value 벡터들로 나중에 투영되었을 때 단어들 간의 거리를 늘릴 수 있다는 점
        - 바로 왼쪽 반은 (크기 256) sine 함수에 의해서 생성되었고, 나머지 오른쪽 반은 또 다른 함수인 cosine 함수에 의해 생성되어 있음
    - 그 후 layer-normalization
- decoding 단계의 각 스텝은 출력 시퀀스의 한 element를 출력합니다
    - 디코딩 스텝은 decoder가 출력을 완료했다는 special 기호인 <end of sentence>를 출력할 때까지 반복
    - self-attention layer은 output sequence 내에서 현재 위치의 이전 위치들에 대해서만 attend 할 수 있습니다. 이것은 self-attention 계산 과정에서 softmax를 취하기 전에 현재 스텝 이후의 위치들에 대해서 masking (즉 그에 대해서 -inf로 치환하는 것)을 해줌으로써 가능해집니다
    -  그 한가지 차이점은 Query 행렬들을 그 밑의 layer에서 가져오고 Key 와 Value 행렬들을 encoder의 출력에서 가져온다는 점
- 여러 개의 decoder를 거치고 난 후에는 소수로 이루어진 벡터 하나가 남게 됩니다. 어떻게 이 하나의 벡터를 단어로 바꿀 수 있을까요? 이것이 바로 마지막에 있는 Linear layer 과 Softmax layer가 하는 일

## Peer session presentation
A - k-size가 recptivesize? 위에 cdim이 kernel size
    - 기존의 1은 무조건 맞춰즌거고, cdim 커널사이즈에 맞추는 거죠
