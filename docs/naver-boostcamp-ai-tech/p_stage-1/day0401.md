# day 0401

## 목표
- 여러가지를 시도해보고 적용해보는 것!
- React.js를 사용하여 업로드 기능 구현

## Office Hour
### Trick form Image Classification

#### Label Smoothing
- 보통 one-hot vector로 0과 1로 구성
- 모델에게 이렇게 확실한 값을 주는 것보다 작은 값으로 스무딩하면 모델이 더 잘 이해한다는 연구가 나옴
    - ex) [1,0] => [0.9,0.1]

#### Knowledge Distillation
- Random Crop?
    - image에 고양이가 없는데 정답을 고양이로 주는 문제가 생김
- Teacher 모델을 학습

#### MixUp
- 개와 고양이가 있다면, 일정한 비율로 합쳐주고, 결과는 smoothing으로 섞인 만큼

#### Cosine Learning Rate Decay
- 학습율이 너무 크거나 작으면 수렴이 어려움
- 학습과정중에 learning rate를 바꾸는 방법을 연구함
    - 0에서 시작하는 것은 후에 설명 예정 (warmup인가?)

#### Experiment
- knowledge distillation하고 label smoothing을 같이 쓰면 student가 학습을 잘 못한다라는 연구가 있음

- segmentation의 경우는, 잘 안될수도 있음

#### After MixUp
- CutMix가 효과가 되게 좋음 (기본으로 사용될정도로 자주 쓰임)
- FMix도 있음
- Tip: 마스크 이미지의 경우... 사람이 중앙에 있으니, CenterCrop으로 해서 CutMix를 해야할듯

#### Other: FixRes
- 이미지를 키우고 자르고, 비율을 맞추는 것

#### DeiT
- ViT 모델을 기반으로 여러 테크닉을 이용한 성능 향상

### Efficient Training

#### Learning Rate
- Batch Size가 클수록 학습이 빨라짐, 대신 수렴하기가 어려워짐.
    - 해결책: batchsize가 클수록 LR을 높임
    - 0.1 * b/256
- Learning Rate를 warm-up
    - learning rate를 서서히 높임

#### Zero Gamma in BatchNorm
- Gamma: scalling vector
- 

#### Low Precision Training
- 기본적으로 32-bit float로 학습을 진행
- 이게 너무 크다 해서 16-bit Float로 진행해보면 어떨까
- Mixed Precision
    - 순전파 & 역전파는 16
    - 가중치 update 32로 진행
    - 32 학습과 유사한 성능을 보여줌

#### Repeated Augmentation
- Common SGD update rule
- Repeated Augmentation
    - 배치 하나에 똑같은 이미지가 있을 수 있는 것을, 각기 다른 방법으로 Augmentation

#### 대회에 어떤 것을 썼는지
- image 분류 대회대회
    - 마스크면 이미지의 중요한 데이터가 어디에 있는지 확인하고 
    - accuracy는 불균형은 반영하지 않으므로, 이거를 잘 처리해야함!
    - f1 score(metric?)을 잘보고 해야하나
- 처음이라면
    - EDA 먼저 진행
    - 기본 베이스를 깔끔하게
        - 설정을 쉽게 바꿀 수 있게
        - 모델을 간단하게 바꿀 수 있게, Optimizer를 쉽게 바꿀 수 있게, Loss function도 쉽게 바꿀수 있게
        - 어떻게 학습되는지 logging을 잘 찍을 수 있게
- 초반에는 작은 모델로 어떤게 잘 되는지 학습해서 테스트해봅
- 작은 사이즈의 이미지로 학습하고, hyperparameter를 학습해서 찾고 여러사이즈 앙상블
- 먼저 lr, batch size, weight decay도 중요하긴 한데,, 좀 더 novelty한
    - loss 또는 knowledge Distillation같은 것들 먼저 적용!
- Adam lR= 3e-4은 통상적인거 먼저 학습

- 초기라면 이거 먼저 해보면 좋을듯
    - 경험상 적용하기 쉽고 성능형사잉 좋은
        1. Label Smoothing
        2. Mixup
        3. CutMix
    LR과 batchNorm 같은 것은 나중에 찾으도록
        LPT를 16bit로 시작
- 이중에 몇개는 코드 몇줄로 가능한듯?
    - label smoothing은 코드 제공!

- 16 bit에서 좋은 성능을 보인 모델이 32 bit에서 동일한 성능 향상을 보이나요??
    - 보통 잘 나옴 (확률이 높음)

- 일반적으로 작은 모델에서 찾은 하이퍼 파라미터가, 큰 모델에서도 좋은 하이퍼 파라미터인가요?
    - 그런듯? 하지만 

- LR 스케줄러 관련해서 조금 더 구체적으로 설명주시면 감사하겠습니다. 특히 데이터와 task의 종류에 따라서 어떤 스케줄러를 사용하는 것이 경험적으로 더 좋다고 느끼셨나요?
    - task가 데이터가 두 개 noisy한거면, label이 이상한거면, LR을 좀 짧게하고 실험을 짧게하는게 좋음
        - 모델이 이상한 부분을 기억하기 떄문에.. 
        - 기억하기전에 끊어버리는게 좋음

- 멘토님께서는 모델의 학습곡선에서 Overfitting / Underfitting을 결정하는 기준을 어떻게 판단하시는지 궁금합니다.
    - early stopping : validation score 특정기간 상승이 안되면 학습을 중단시킴

- 추론 단계에서의 이미지 Transform은 꼭 Resize, Centercrop 등으로 정형화하여 사용해야하나요?
    - 내일 배울듯
    - TTA라는 거를 내일 배움
        - 여러 transforms을 적용한 것을 평균내어 결과낼수도 있음!
- lr scheduler 관련해서 더 질문 드리고 싶습니다! cosine scheduler를 적용해보고 싶은데, 적은 epoch에도 수렴하더라구요. 이런 경우에 코사인 주기와 같은 값을 넣기가 어려운데 어떤 팁을 주실 수 있으실까요..? (lr은 0.0001을 사용했습니다.)

- 이번 마스크 data에서는 사람이 봐도 55~60 대 사람들 나이가 분간하기가 힘든데 이런 경우 모델은 어떤 피쳐로 이걸 분류하나요?
    - 고민하시고 알려주실듯
    - 고지형: 나이문제는 회귀문제를 바꿔서 풀면 어떨까?
        - regression으로 주면 범위로 주는 정보량보다 배울 수 있는게 많을 것 같음

- 현재 마스크 데이터에 라벨과는 다른 내용이(남자로 라벨되어있는데 여자로 되어있는 경우, incorrect_mask랑 nomal마스크랑 라벨링이 다른 경우) 발견된다고 피어세션에 들었습니다. 이럴 경우.. 일일이 보는 방법 말고 마스터 님은 어떤 식으로 라벨링이 잘못된것을 찾으실지 궁금합니다!

